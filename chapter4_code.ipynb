{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "githubからデータを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Apress/mastering-ml-w-python-in-six-steps.git\n",
    "!ln -s mastering-ml-w-python-in-six-steps/Chapter_4_Code/Code/Data Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warinigの非表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1.  データの読み込みとクラス分布の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "print (df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-2. ロジスティック回帰モデルの構築と性能の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:8]     # 独立変数\n",
    "y = df['class']     # 従属変数\n",
    "\n",
    "# 訓練データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# ロジスティック回帰モデルをインスタンス化して適合\n",
    "model = LogisticRegression(max_iter=150)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# 訓練セットのクラスラベルを予測する。predict関数は確率が0.5より大きい値を1 か 0に変換する\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# クラス確率の生成\n",
    "# probs配列には2つの要素が返されることに注意\n",
    "# 1番目の要素は負のクラスの確率\n",
    "# 2番目の要素は正のクラスの確率\n",
    "probs = model.predict_proba(X_train)\n",
    "y_pred_prob = probs[:, 1]\n",
    "\n",
    "# 評価指標の生成\n",
    "print (\"Accuracy: \", metrics.accuracy_score(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-3.  最適なカットオフポイントを見つける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 偽陽性、真陽性率の抽出\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred_prob)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n",
    "i = np.arange(len(tpr)) # Dataframeのインデックス\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series\n",
    "(tpr, index = i),'1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series\n",
    "(tpr - (1-fpr), index = i),'thresholds' : pd.Series(thresholds, \n",
    "index = i)})\n",
    "roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "# tprと1-fprプロットして比較\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'], label='tpr')\n",
    "plt.plot(roc['1-fpr'], color = 'red', label='1-fpr')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-4. 最適な確率カットオフを見つけるための関数例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\"\n",
    "    イベントレートに関連する分類モデルの最適な確率カットオフポイントを見つけるためのパラメータ\n",
    "    ----------\n",
    "    target: 行が観測値。従属データまたは目的データを持つ行列\n",
    "    predicted : 予測されたデータを持つ行列\n",
    "    返り値\n",
    "    最適なカットオフ値を持つリスト\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold']) \n",
    "\n",
    "# 最適な確率の閾値を見つける\n",
    "# Note: probs[:, 1] は正のラベルである確率を持つ\n",
    "threshold = Find_Optimal_Cutoff(y_train, probs[:, 1])\n",
    "print (\"Optimal Probability Threshold: \", threshold)\n",
    "\n",
    "# 予測確率に閾値を適用する\n",
    "y_pred_optimal = np.where(y_pred_prob >= threshold, 1, 0)\n",
    "\n",
    "# 通常のアプローチと最適なカットオフの精度を比較する\n",
    "print (\"\\nNormal - Accuracy: \", metrics.accuracy_score(y_train, y_pred))\n",
    "print (\"Optimal Cutoff - Accuracy: \", metrics.accuracy_score(y_train, y_pred_optimal))\n",
    "print (\"\\nNormal - Confusion Matrix: \\n\", metrics.confusion_matrix(y_train, y_pred))\n",
    "print (\"Optimal - Cutoff Confusion Matrix: \\n\", metrics.confusion_matrix(y_train, y_pred_optimal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-5. レアケースと不均衡データセットの取り扱い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 単純化のために2つの特徴でデータセットを生成する\n",
    "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
    "                           \n",
    "n_redundant=0, weights=[0.9, 0.1], random_state=2017)\n",
    "print (\"Positive class: \", y.tolist().count(1))\n",
    "print (\"Negative class: \", y.tolist().count(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imbalanced-learnをpip install しておく\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ランダムアンダーサンプリング\n",
    "rus = RandomUnderSampler()\n",
    "X_RUS, y_RUS = rus.fit_resample(X, y)\n",
    "\n",
    "# ランダムオーバーサンプリング\n",
    "ros = RandomOverSampler()\n",
    "X_ROS, y_ROS = ros.fit_resample(X, y)\n",
    "\n",
    "# SMOTE\n",
    "sm = SMOTE()\n",
    "X_SMOTE, y_SMOTE = sm.fit_resample(X, y)\n",
    "\n",
    "# 元データとリサンプリング結果を比較\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(X[y==0,0], X[y==0,1], marker='o', color='blue')\n",
    "plt.scatter(X[y==1,0], X[y==1,1], marker='+', color='red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Original: 1=%s and 0=%s' %(y.tolist().count(1), y.tolist().count(0)))\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(X_RUS[y_RUS==0,0], X_RUS[y_RUS==0,1], marker='o', color='blue')\n",
    "plt.scatter(X_RUS[y_RUS==1,0], X_RUS[y_RUS==1,1], marker='+', color='red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y2')\n",
    "plt.title('Random Under-sampling: 1=%s and 0=%s' %(y_RUS.tolist().count(1), y_RUS.tolist().count(0)))\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(X_ROS[y_ROS==0,0], X_ROS[y_ROS==0,1], marker='o', color='blue')\n",
    "plt.scatter(X_ROS[y_ROS==1,0], X_ROS[y_ROS==1,1], marker='+', color='red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2') \n",
    "plt.title('Random over-sampling: 1=%s and 0=%s' %(y_ROS.tolist().count(1), y_ROS.tolist().count(0)))\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(X_SMOTE[y_SMOTE==0,0], X_SMOTE[y_SMOTE==0,1], marker='o', color='blue')\n",
    "plt.scatter(X_SMOTE[y_SMOTE==1,0], X_SMOTE[y_SMOTE==1,1], marker='+', color='red')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y2')\n",
    "plt.title('SMOTE: 1=%s and 0=%s' %(y_SMOTE.tolist().count(1), y_SMOTE.tolist().count(0)))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-6. 様々なリサンプリング手法を用いたモデルの構築とパフォーマンスの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_RUS_train, X_RUS_test, y_RUS_train, y_RUS_test = train_test_split(X_RUS, y_RUS, test_size=0.3, random_state=2017)\n",
    "X_ROS_train, X_ROS_test, y_ROS_train, y_ROS_test = train_test_split(X_ROS, y_ROS, test_size=0.3, random_state=2017)\n",
    "X_SMOTE_train, X_SMOTE_test, y_SMOTE_train, y_SMOTE_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.3, random_state=2017)\n",
    "\n",
    "# 決定木の構築\n",
    "clf = tree.DecisionTreeClassifier(random_state=2017)\n",
    "clf_rus = clf.fit(X_RUS_train, y_RUS_train)\n",
    "clf_ros = clf.fit(X_ROS_train, y_ROS_train)\n",
    "clf_smote = clf.fit(X_SMOTE_train, y_SMOTE_train)\n",
    "\n",
    "# モデルの性能を評価\n",
    "print (\"\\nRUS - Train AUC : \",metrics.roc_auc_score(y_RUS_train, clf.predict(X_RUS_train)))\n",
    "print (\"RUS - Test AUC : \",metrics.roc_auc_score(y_RUS_test, clf.predict\n",
    "(X_RUS_test)))\n",
    "print (\"ROS - Train AUC : \",metrics.roc_auc_score(y_ROS_train, clf.predict(X_ROS_train)))\n",
    "print (\"ROS - Test AUC : \",metrics.roc_auc_score(y_ROS_test, clf.predict(X_ROS_test)))\n",
    "print (\"\\nSMOTE - Train AUC : \",metrics.roc_auc_score(y_SMOTE_train, clf.predict(X_SMOTE_train)))\n",
    "print (\"SMOTE - Test AUC : \",metrics.roc_auc_score(y_SMOTE_test, clf.predict(X_SMOTE_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-7 k分割交差検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,:8].values     # 独立変数\n",
    "y = df['class'].values     # 従属変数\n",
    "\n",
    "# データの正規化\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "# 学習データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)\n",
    "\n",
    "# 決定木を構築\n",
    "# clf = tree.DecisionTreeClassifier(random_state=2017)\n",
    "clf = LogisticRegression(random_state=2017)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# 10分割の交差検証でモデルを評価\n",
    "train_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "test_scores = cross_val_score(clf, X_test, y_test, scoring='accuracy', cv=5)\n",
    "print (\"Train Fold AUC Scores: \", train_scores)\n",
    "print (\"Train CV AUC Score: \", train_scores.mean())\n",
    "print (\"\\nTest Fold AUC Scores: \", test_scores)\n",
    "print (\"Test CV AUC Score: \", test_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-8.  層化K-分割交差検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "k = 0\n",
    "for (train, test) in kfold.split(X_train, y_train):\n",
    "    clf.fit(X_train[train], y_train[train])\n",
    "    train_score = clf.score(X_train[train], y_train[train])\n",
    "    train_scores.append(train_score)\n",
    "    # テストデータでのスコア\n",
    "    test_score = clf.score(X_train[test], y_train[test])\n",
    "    test_scores.append(test_score)\n",
    "    k += 1\n",
    "    print('Fold: %s, Class dist.: %s, Train Acc: %.3f, Test Acc: %.3f'% (k, np.bincount(y_train[train]), train_score, test_score))\n",
    "print('\\nTrain CV accuracy: %.3f' % (np.mean(train_scores)))\n",
    "print('Test CV accuracy: %.3f' % (np.mean(test_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-9 層化K-分割交差検証のためのROC曲線のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "from numpy import interp as np_interp\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\n",
    "lw = 2\n",
    "\n",
    "i = 0\n",
    "\n",
    "for (train, test), color in zip(kfold.split(X, y), colors):\n",
    "    probas_ = clf.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # ROC曲線の計算\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    mean_tpr += np_interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=lw, color=color,label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    i += 1\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=lw, color='k', label='Luck')\n",
    "mean_tpr /= kfold.get_n_splits(X, y)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate') \n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-10. 決定木とバギングの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定木による分類\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,:8].values     # 独立変数\n",
    "y = df['class'].values     # 従属変数\n",
    "\n",
    "# 正規化\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 学習データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 100\n",
    "\n",
    "# 決定木，5回の畳み込みによるクロスバリデーション\n",
    "clf_DT = DecisionTreeClassifier(random_state=2019).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT, X_train,y_train, cv=kfold)\n",
    "print (\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print (\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n",
    "\n",
    "# バギングを用いて100個の決定木モデルを構築し，平均/多数決の予測を行う\n",
    "clf_DT_Bag = BaggingClassifier(base_estimator=clf_DT, n_estimators=num_trees, random_state=2019).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT_Bag, X_train, y_train, cv=kfold)\n",
    "print (\"\\nDecision Tree (Bagging) - Train : \", results.mean())\n",
    "print (\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(clf_DT_Bag.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-11.  決定木における特徴量の重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf_DT.feature_importances_\n",
    "\n",
    "# 重要度の高いものから相対的に重要度を上げる\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, df.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-12. ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "clf_RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_RF, X_train, y_train, cv=kfold)\n",
    "print (\"Random Forest (Bagging) - Train : \", results.mean())\n",
    "print (\"Random Forest (Bagging) - Test : \", metrics.accuracy_score(clf_RF.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-13. エクストラツリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "clf_ET = ExtraTreesClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_ET, X_train, y_train, cv=kfold)\n",
    "print (\"ExtraTree - Train : \", results.mean())\n",
    "print (\"ExtraTree - Test : \", metrics.accuracy_score(clf_ET.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-14. 決定境界のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# PCA\n",
    "X = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 100\n",
    "\n",
    "seed = 2019 \n",
    "\n",
    "# 決定木\n",
    "clf_DT = DecisionTreeClassifier(random_state=seed).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT, X_train,y_train, cv=kfold)\n",
    "print(\"Decision Tree (stand alone) - Train : \", results.mean())\n",
    "print(\"Decision Tree (stand alone) - Test : \", metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n",
    "\n",
    "# バギング\n",
    "clf_DT_Bag = BaggingClassifier(base_estimator=clf_DT, n_estimators=num_trees, random_state=seed).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT_Bag, X_train, y_train, cv=kfold)\n",
    "print(\"Decision Tree (Bagging) - Train : \", results.mean())\n",
    "print(\"Decision Tree (Bagging) - Test : \", metrics.accuracy_score(clf_DT_Bag.predict(X_test), y_test))\n",
    "\n",
    "# ランダムフォレスト\n",
    "clf_RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_RF, X_train, y_train, cv=kfold)\n",
    "print(\"Random Forest - Train : \", results.mean())\n",
    "print(\"Random Forest  - Test : \", metrics.accuracy_score(clf_RF.predict(X_test), y_test))\n",
    "\n",
    "#エクストラツリー\n",
    "clf_ET = ExtraTreesClassifier(n_estimators=num_trees).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_ET, X_train, y_train, cv=kfold)\n",
    "print(\"ExtraTree - Train : \", results.mean())\n",
    "print(\"ExtraTree - Test : \", metrics.accuracy_score(clf_ET.predict(X_test), y_test))\n",
    "\n",
    "def plot_decision_regions(X, y, classifier):\n",
    "    h = .02  # メッシュサイズの設定\n",
    "    # マーカーのカラーマップの設定\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # 決定境界のプロット設定\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=colors[idx],\n",
    "                    marker=markers[idx], label=cl)\n",
    "\n",
    "# 決定境界のプロット\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(221)\n",
    "plot_decision_regions(X, y, clf_DT)\n",
    "plt.title('Decision Tree (Stand alone)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.subplot(222)\n",
    "plot_decision_regions(X, y, clf_DT_Bag)\n",
    "plt.title('Decision Tree (Bagging - 100 trees)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(223)\n",
    "plot_decision_regions(X, y, clf_RF)\n",
    "plt.title('RandomForest Tree (100 trees)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(224)\n",
    "plot_decision_regions(X, y, clf_ET)\n",
    "plt.title('Extream Random Tree (100 trees)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-15. 決定木とAdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定木による分類\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "\n",
    "# 弱い特徴量を使って決定木を作成する\n",
    "X = df[['age','serum_insulin']]     # 独立変数\n",
    "y = df['class'].values              # 従属変数\n",
    "\n",
    "# 正規化\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 学習データセットとテストデータセットに分けてモデルを評価する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 100\n",
    "\n",
    "# 5倍のクロスバリデーションを用いた決定木\n",
    "# より多くの不純物を含む葉を得るためにmax_depthを1に制限する\n",
    "clf_DT = DecisionTreeClassifier(max_depth=1, random_state=2019).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT, X_train,y_train, cv=kfold.split(X_train, y_train))\n",
    "print(\"Decision Tree (stand alone) - CV Train : %.2f\" % results.mean())\n",
    "print(\"Decision Tree (stand alone) - Test : %.2f\" % metrics.accuracy_score(clf_DT.predict(X_train), y_train))\n",
    "print(\"Decision Tree (stand alone) - Test : %.2f\" % metrics.accuracy_score(clf_DT.predict(X_test), y_test))\n",
    "\n",
    "# Adaptive Boostingを100回繰り返す\n",
    "clf_DT_Boost = AdaBoostClassifier(base_estimator=clf_DT, n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train,y_train)\n",
    "results = model_selection.cross_val_score(clf_DT_Boost, X_train, y_train, cv=kfold.split(X_train, y_train))\n",
    "print(\"\\nDecision Tree (AdaBoosting) - CV Train : %.2f\" % results.mean())\n",
    "print(\"Decision Tree (AdaBoosting) - Train : %.2f\" % metrics.accuracy_score(clf_DT_Boost.predict(X_train), y_train))\n",
    "print(\"Decision Tree (AdaBoosting) - Test : %.2f\" % metrics.accuracy_score(clf_DT_Boost.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-16. 勾配ブースティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Gradient Boostingを100回繰り返す\n",
    "clf_GBT = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv=kfold)\n",
    "print (\"\\nGradient Boosting - CV Train : %.2f\" % results.mean())\n",
    "print (\"Gradient Boosting - Train : %.2f\" % metrics.accuracy_score(clf_GBT.predict(X_train), y_train))\n",
    "print (\"Gradient Boosting - Test : %.2f\" % metrics.accuracy_score(clf_GBT.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "df= pd.read_csv('Data/digit.csv')\n",
    "X = df.iloc[:,1:17].values\n",
    "y = df['lettr'].values\n",
    "\n",
    "# 訓練データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 10\n",
    "clf_GBT = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train, y_train)\n",
    "results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv=kfold)\n",
    "print (\"\\nGradient Boosting - Train : %.2f\" % metrics.accuracy_score\n",
    "(clf_GBT.predict(X_train), y_train))\n",
    "print (\"Gradient Boosting - Test : %.2f\" % metrics.accuracy_score\n",
    "(clf_GBT.predict(X_test), y_test))\n",
    "\n",
    "# 'T'という文字を予測して、予測精度がブースティングの反復ごとにどのように変化するかをみてみよう\n",
    "X_valid= (2,8,3,5,1,8,13,0,6,6,10,8,0,8,0,8)\n",
    "print (\"Predicted letter: \", clf_GBT.predict([X_valid]))\n",
    "\n",
    "# 各段階ではブースティングの各反復で予測される確率を与える\n",
    "stage_preds = list(clf_GBT.staged_predict_proba([X_valid]))\n",
    "final_preds = clf_GBT.predict_proba([X_valid])\n",
    "\n",
    "# プロット\n",
    "x = range(1,27)\n",
    "label = np.unique(df['lettr'])\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(131)\n",
    "plt.bar(x, stage_preds[0][0], align='center')\n",
    "plt.xticks(x, label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Prediction Probability')\n",
    "plt.title('Round One')\n",
    "plt.autoscale()\n",
    "plt.subplot(132)\n",
    "plt.bar(x, stage_preds[5][0],align='center')\n",
    "plt.xticks(x, label)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Prediction Probability')\n",
    "plt.title('Round Five')\n",
    "plt.autoscale()\n",
    "plt.subplot(133)\n",
    "plt.bar(x, stage_preds[9][0],align='center')\n",
    "plt.xticks(x, label)\n",
    "plt.autoscale()\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Prediction Probability')\n",
    "plt.title('Round Ten')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-17. sklearnのラッパーを使ったxgboostr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboostをpip install しておく\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "predictors = ['age','serum_insulin']\n",
    "target = 'class'\n",
    "\n",
    "# 一般的な前処理としてラベルのエンコーディングと欠損値の処理を行う\n",
    "from sklearn import preprocessing\n",
    "for f in df.columns:\n",
    "    if df[f].dtype=='object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df[f].values))\n",
    "        df[f] = lbl.transform(list(df[f].values))\n",
    "\n",
    "df.fillna((-999), inplace=True)\n",
    "\n",
    "# 決定木を構築するために弱い特徴量をいくつか使ってみよう\n",
    "X = df[['age','serum_insulin']] # 独立変数\n",
    "y = df['class'].values          # 従属変数\n",
    "\n",
    "# 標準化\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 弱い特徴量を使って木を作成する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "num_rounds = 100\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "clf_XGB = XGBClassifier(n_estimators = num_rounds, objective= 'binary:logistic',seed=2017)\n",
    "\n",
    "# early_stopping_rounds を使用し，スコアが向上しなかった場合に cv を停止する\n",
    "clf_XGB.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test, y_test)], verbose=False)\n",
    "results = model_selection.cross_val_score(clf_XGB, X_train,y_train, cv=kfold)\n",
    "print (\"\\nxgBoost - CV Train : %.2f\" % results.mean())\n",
    "print (\"xgBoost - Train : %.2f\" % metrics.accuracy_score(clf_XGB.predict\n",
    "(X_train), y_train))\n",
    "print (\"xgBoost - Test : %.2f\" % metrics.accuracy_score(clf_XGB.predict\n",
    "(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-18. Pythonのネイティブパッケージを使ったxgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(X_train, label=y_train, missing=-999)\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test, missing=-999)\n",
    "\n",
    "# xgboostのパラメータを設定\n",
    "param = {'max_depth': 3,  # 各木の最大の深さ\n",
    "         'objective': 'binary:logistic'}\n",
    "clf_xgb_cv = xgb.cv(param, xgtrain, num_rounds,\n",
    "                    stratified=True,\n",
    "                    nfold=5,\n",
    "                    early_stopping_rounds=20,\n",
    "                    seed=2017)\n",
    "print (\"Optimal number of trees/estimators is %i\" % clf_xgb_cv.shape[0])\n",
    "watchlist  = [(xgtest,'test'), (xgtrain,'train')]\n",
    "clf_xgb = xgb.train(param, xgtrain,clf_xgb_cv.shape[0], watchlist)\n",
    "\n",
    "# predict関数で確率を生成\n",
    "# 0.5のカットオフを使って確率をクラスラベルに変換\n",
    "y_train_pred = (clf_xgb.predict(xgtrain, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "y_test_pred = (clf_xgb.predict(xgtest, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\n",
    "print (\"XGB - Train : %.2f\" % metrics.accuracy_score(y_train_pred, y_train))\n",
    "print (\"XGB - Test : %.2f\" % metrics.accuracy_score(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-19. アンサンブルモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlxtendをpip install しておく\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 擬似乱数の生成\n",
    "np.random.seed(2017)\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# 現在はsklearnではなくmlxtendの一部として利用可能\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,:8]     # 独立変数\n",
    "y = df['class']       # 従属変数\n",
    "\n",
    "# 訓練データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)\n",
    "LR = LogisticRegression(random_state=2017, max_iter=200)\n",
    "RF = RandomForestClassifier(n_estimators = 100, random_state=2017)\n",
    "SVM = SVC(random_state=0, probability=True)\n",
    "KNC = KNeighborsClassifier()\n",
    "DTC = DecisionTreeClassifier()\n",
    "ABC = AdaBoostClassifier(n_estimators = 100)\n",
    "BC = BaggingClassifier(n_estimators = 100)\n",
    "GBC = GradientBoostingClassifier(n_estimators = 100)\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([LR, RF, SVM, KNC, DTC, ABC, BC, GBC],\n",
    "                      ['Logistic Regression',\n",
    "                       'Random Forest',\n",
    "                       'Support Vector Machine',\n",
    "                       'KNeighbors',\n",
    "                       'Decision Tree',\n",
    "                       'Ada Boost',\n",
    "                       'Bagging',\n",
    "                       'Gradient Boosting']):\n",
    "    \n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy') \n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)\n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-20. アンサンブル投票モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### アンサンブル投票\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "ECH = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='hard')\n",
    "ECS = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='soft', weights=[1,1,1])\n",
    "for clf, label in zip([ECH, ECS],\n",
    "                      ['Ensemble Hard Voting',\n",
    "                       'Ensemble Soft Voting']):\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)\n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict\n",
    "    (X_test), y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-21. スタッキングモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラス分類\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "seed = 2019\n",
    "np.random.seed(seed)  # 乱数の初期化\n",
    "# データの読み込み\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,0:8] # 独立変数\n",
    "y = df['class'].values     # 従属変数\n",
    "\n",
    "# 正規化\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# 訓練データセットとテストデータセットに分けて評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 10\n",
    "verbose = True # to print the progress\n",
    "clfs = [KNeighborsClassifier(),\n",
    "        RandomForestClassifier(n_estimators=num_trees, random_state=seed),\n",
    "        \n",
    "GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)]\n",
    "# ブレンド用の訓練データセットとテストデータセットの作成\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "print('5-fold cross validation:\\n')\n",
    "for i, clf in enumerate(clfs):\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    print(\"##### Base Model %0.0f #####\" % i)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_train), y_train)))\n",
    "    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]\n",
    "    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]  \n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n",
    "\n",
    "print (\"##### Meta Model #####\")\n",
    "clf = LogisticRegression()\n",
    "scores = model_selection.cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring='accuracy')\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-22.  ハイパーパラメータ調整のためのグリッドサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "seed = 2017\n",
    "# データの読み込み\n",
    "\n",
    "df = pd.read_csv(\"Data/Diabetes.csv\")\n",
    "X = df.iloc[:,:8].values     # 独立変数\n",
    "y = df['class'].values       # 従属変数\n",
    "\n",
    "# 正規化\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 訓練データセットとテストデータセットに分けてモデルを評価\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5)\n",
    "num_trees = 100\n",
    "clf_rf = RandomForestClassifier(random_state=seed).fit(X_train, y_train)\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 250, 500, 750, 1000],\n",
    "    'criterion':  ['gini', 'entropy'],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "# verbose = 10を設定するとタスクが10回完了するごとに進捗状況が表示される\n",
    "grid = GridSearchCV(clf_rf, rf_params, scoring='roc_auc', cv=kfold, verbose=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print ('Best Parameters: ', grid.best_params_)\n",
    "results = model_selection.cross_val_score(grid.best_estimator_, X_train,y_train, cv=kfold)\n",
    "print (\"Accuracy - Train CV: \", results.mean())\n",
    "print (\"Accuracy - Train : \", metrics.accuracy_score(grid.best_estimator_.predict(X_train), y_train))\n",
    "print (\"Accuracy - Test : \", metrics.accuracy_score(grid.best_estimator_.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-23.ハイパーパラメータ調整のためのランダムサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# サンプリングするパラメータと分布を指定する\n",
    "param_dist = {'n_estimators':sp_randint(100,1000),\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [None, 1, 3, 5, 7, 9]\n",
    "             }\n",
    "# ランダムサーチを実行\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf_rf, param_distributions=param_dist, cv=kfold, n_iter=n_iter_search, verbose=10, n_jobs=-1, random_state=seed)\n",
    "random_search.fit(X_train, y_train)\n",
    "print ('Best Parameters: ', random_search.best_params_)\n",
    "results = model_selection.cross_val_score(random_search.best_estimator_, X_train,y_train, cv=kfold)\n",
    "print (\"Accuracy - Train CV: \", results.mean())\n",
    "print (\"Accuracy - Train : \", metrics.accuracy_score(random_search.best_estimator_.predict(X_train), y_train))\n",
    "print (\"Accuracy - Test : \", metrics.accuracy_score(random_search.best_estimator_.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4-24. ハイパーパラメータ調整のためのベイズ最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baysian-optimizationをpip installしておく\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt.util import Colours\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "def rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n",
    "    \"\"\"\n",
    "    ランダムフォレストのクロスバリデーション\n",
    "\n",
    "    この関数はn_estimators、min_samples_split、max_featuresをパラメータとして，ランダムフォレスト分類器をインスタンス化する。これにデータとターゲットを組み合わせてクロスバリデーションを行う。ここでの我々の目標はlog lossを最小化するn_estimators, min_samples_split, max_featuresの組み合わせを見つけることである。\n",
    "    \"\"\"\n",
    "    estimator = RFC(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_features=max_features,\n",
    "        random_state=2\n",
    "    )\n",
    "    cval = cross_val_score(estimator, data, targets,\n",
    "                           scoring='neg_log_loss', cv=4)\n",
    "    return cval.mean()\n",
    "def optimize_rfc(data, targets):\n",
    "    \"\"\"ランダムフォレストのパラメータにベイズ最適化を適用.\"\"\"\n",
    "    def rfc_crossval(n_estimators, min_samples_split, max_features):\n",
    "        \"\"\" \n",
    "        RandomForestクロスバリデーションのラッパー\n",
    "        n_estimatorsとmin_samples_splitを渡す前に，integerにキャストしていることに注目してほしい。さらにmax_featuresが(0, 1)の範囲外の値を取ることを避けるためにそれに応じてキャップされていることも確認している\n",
    "        \"\"\"\n",
    "        return rfc_cv(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=max(min(max_features, 0.999), 1e-3),\n",
    "            data=data,\n",
    "            targets=targets,\n",
    "        )\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (10, 250),\n",
    "            \"min_samples_split\": (2, 25),\n",
    "            \"max_features\": (0.1, 0.999),\n",
    "        },\n",
    "        random_state=1234,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(n_iter=10)\n",
    "    print(\"Final result:\", optimizer.max)\n",
    "    return optimizer\n",
    "print(Colours.green(\"--- Optimizing Random Forest ---\"))\n",
    "optimize_rfc(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-25. ウェーブレット変換の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "from statsmodels.robust import mad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Data/Temperature.csv')\n",
    "\n",
    "# ウェーブレット変換を用いてセンサデータをノイズ除去する\n",
    "def wp_denoise(df):\n",
    "    for column in df:\n",
    "        x = df[column]\n",
    "        wp = pywt.WaveletPacket(data=x, wavelet='db7', mode='symmetric')\n",
    "        new_wp = pywt.WaveletPacket(data=None, wavelet='db7', mode='sym')\n",
    "        for i in range(wp.maxlevel):\n",
    "            nodes = [node.path for node in wp.get_level(i, 'natural')]\n",
    "           # ハイパス、ローパス信号の除去\n",
    "            for node in nodes:\n",
    "                sigma = mad(wp[node].data)\n",
    "                uthresh = sigma * np.sqrt( 2*np.log( len( wp[node].data ) ) )\n",
    "                new_wp[node] = pywt.threshold(wp[node].data, value=uthresh, mode='soft')\n",
    "    y = new_wp.reconstruct(update=False)[:len(x)]\n",
    "    df[column] = y\n",
    "    return df\n",
    "    \n",
    "# センサーデータのノイズを除去\n",
    "df_denoised = wp_denoise(df.iloc[:,3:4])\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "plt.figure(1)\n",
    "ax1 = plt.subplot(221)\n",
    "df['4030CFDC'].plot(ax=ax1, figsize=(8, 8), title='Signal with noise')\n",
    "ax2 = plt.subplot(222)\n",
    "df_denoised['4030CFDC'].plot(ax=ax2, figsize=(8, 8), title='Signal without noise')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f32249afd32d99e95f28ac6a48c360a6a1e4371687b828dd8d508d7eb90d6ec8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
